{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deba117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed997c",
   "metadata": {},
   "source": [
    "transformers#==4.37.2\n",
    "accelerate#==0.25.0\n",
    "datasets\n",
    "peft#==0.6.2\n",
    "bitsandbytes#==0.41.0\n",
    "jiwer\n",
    "wandb\n",
    "triton#==2.0.0\n",
    "einops#>=0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ae21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = 'slip-ml'\n",
    "role = 'arn:aws:iam::438465160412:role/Sagemaker'\n",
    "project_name = 'llama-phoneme'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name = \"huggingface\"\n",
    "region_name = \"us-east-1\"\n",
    "session = boto3.session.Session()\n",
    "secretsmanager = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "get_secret_value_response = secretsmanager.get_secret_value(SecretId=secret_name)\n",
    "secret = get_secret_value_response['SecretString']\n",
    "api_key = json.loads(secret)[\"API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances_gpus = {\n",
    "    \"ml.g5.2xlarge\": 1,\n",
    "    \"ml.g5.12xlarge\": 4,\n",
    "    \"ml.p4d.24xlarge\": 8,\n",
    "    \"ml.p5.48xlarge\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3f6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.image_uris.get_training_image_uri(framework='pytorch',\n",
    "                            region=sagemaker_session.boto_region_name, \n",
    "                            instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(framework='pytorch',\n",
    "                            region=sagemaker_session.boto_region_name,\n",
    "                            instance_type=instance_type,\n",
    "                            image_scope='training'\n",
    "                             )\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"finetune_llama.py\",\n",
    "    source_dir=\"source\",\n",
    "    role=role,\n",
    "    base_job_name=project_name,\n",
    "    instance_count=1,  \n",
    "    instance_type=instance_type,\n",
    "    framework_version='2.2.0',\n",
    "    py_version=\"py310\",\n",
    "    #distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    distribution={\n",
    "            'smdistributed': {\n",
    "                'dataparallel': {\n",
    "                    'enabled': True,\n",
    "                    'parameters': {\n",
    "                        'sharded_data_parallel_degree': str(training_instances_gpus[instance_type]),\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    hyperparameters={\n",
    "        \"batch-size\": 2,\n",
    "        \"epochs\": 7,\n",
    "        \"lr\": 3e-4,\n",
    "        \"seed\": 1,\n",
    "        \"project-name\": f\"{project_name}\",\n",
    "        \"bucket\": f\"{bucket}\",\n",
    "    },\n",
    "    environment={\n",
    "        \"HF_TOKEN\": \"\" + api_key,\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    volume_size=100,\n",
    "    output_path=f's3://{bucket}/models/{project_name}',\n",
    "    code_location=f's3://{bucket}/model-building/{project_name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ec546",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': f's3://{bucket}/data/transcriptions/train/',\n",
    "               'test': f's3://{bucket}/data/transcriptions/test/'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e273b5a",
   "metadata": {},
   "source": [
    "The error you're encountering in AWS SageMaker, `RuntimeError: [2]: params[0] in this process with sizes [0] appears not to match sizes of the same param in process 0`, indicates a mismatch in the model parameter sizes across different processes during distributed training with PyTorch's Fully Sharded Data Parallel (FSDP). This typically happens when the model parameters are not consistently initialized or synchronized across all processes (GPUs) in a distributed setup. Below, I'll analyze the issue based on your provided code (`finetune_llama.py`) and suggest fixes to resolve this error.\n",
    "\n",
    "### Root Cause Analysis\n",
    "The error occurs in the `torch.nn.parallel.DistributedDataParallel` (DDP) or FSDP setup, specifically during parameter verification across processes. The key points from the stack trace and code are:\n",
    "\n",
    "1. **Parameter Size Mismatch**: The error suggests that the first parameter (`params[0]`) in process 2 (or process 6 in another instance) has a size of `[0]`, while process 0 has a different size. This indicates that the model parameters are not identical across all ranks, which is a requirement for distributed training.\n",
    "\n",
    "2. **FSDP and LoRA**: Your code uses FSDP with LoRA (Low-Rank Adaptation) via the `peft` library. LoRA modifies the model by adding adapter layers, which can sometimes cause inconsistencies if not applied uniformly across all processes. The error likely stems from the model initialization or LoRA application phase.\n",
    "\n",
    "3. **SageMaker and SMDDP Backend**: You're using the `smddp` backend (`smdistributed.dataparallel.torch.torch_smddp`) for distributed training in SageMaker. This backend integrates with PyTorch's distributed training but may have specific requirements for model synchronization.\n",
    "\n",
    "4. **Parameter Synchronization**: The code attempts to synchronize parameters using `dist.broadcast` for `requires_grad` parameters after applying LoRA and moving the model to the GPU. However, the synchronization might not be covering all parameters or might be failing due to incorrect handling of the model state.\n",
    "\n",
    "5. **Potential Issues**:\n",
    "   - **LoRA Application Inconsistency**: The LoRA adapters might not be applied consistently across all ranks, leading to different parameter counts.\n",
    "   - **FSDP Wrapping**: The model is wrapped with FSDP after LoRA is applied, but the parameter synchronization might not account for FSDP's sharding.\n",
    "   - **Device Placement**: Moving the model to a specific GPU (`cuda:{device_id}`) before FSDP wrapping and synchronization could cause issues if the model state is not properly aligned.\n",
    "   - **Dataset or Model Initialization**: If the model or dataset sharding is not deterministic across ranks, it could indirectly affect the training setup.\n",
    "\n",
    "### Suggested Fixes\n",
    "Here are step-by-step fixes to address the parameter mismatch error:\n",
    "\n",
    "#### 1. **Ensure Consistent Model Initialization Across Ranks**\n",
    "The model must be initialized identically on all ranks before applying FSDP or LoRA. The current code loads the model and applies LoRA before synchronization, which is correct, but we need to ensure that no rank-specific operations interfere.\n",
    "\n",
    "**Fix**: Move the model to the CPU initially and only move it to the GPU after FSDP wrapping to avoid device-specific issues. Also, ensure the model is fully synchronized before FSDP wrapping.\n",
    "\n",
    "**Modified Code** (in the `train` function, replace the model loading and LoRA application section):\n",
    "\n",
    "```python\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    "    use_fast=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model configuration\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.use_cache = False\n",
    "\n",
    "# Determine compute dtype\n",
    "compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "logger.info(f\"Using {compute_dtype} as compute dtype\")\n",
    "\n",
    "# Load model on CPU to ensure consistency\n",
    "logger.info(f\"Loading base model: {model_name}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    torch_dtype=compute_dtype,\n",
    "    token=os.environ.get(\"HF_TOKEN\"),\n",
    "    device_map=None  # Keep on CPU initially\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False\n",
    ")\n",
    "logger.info(\"Applying LoRA adapters to model\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Synchronize model parameters across all ranks\n",
    "if dist.is_initialized():\n",
    "    logger.info(f\"Synchronizing model parameters across processes (rank {rank})\")\n",
    "    for param in model.parameters():\n",
    "        dist.broadcast(param.data, src=0)  # Broadcast all parameters, not just requires_grad\n",
    "    dist.barrier()\n",
    "    logger.info(f\"Model parameters synchronized on rank {rank}\")\n",
    "\n",
    "# Move model to GPU after synchronization\n",
    "device_id = local_rank\n",
    "model = model.to(f\"cuda:{device_id}\")\n",
    "logger.info(f\"Moved model to device cuda:{device_id}\")\n",
    "\n",
    "# Apply FSDP wrapping\n",
    "fsdp_kwargs = {\n",
    "    \"auto_wrap_policy\": auto_wrap_policy,\n",
    "    \"mixed_precision\": mixed_precision_policy,\n",
    "    \"sharding_strategy\": sharding_strategy,\n",
    "    \"limit_all_gathers\": True,\n",
    "    \"cpu_offload\": cpu_offload,\n",
    "    \"device_id\": torch.cuda.current_device(),\n",
    "    \"use_orig_params\": True\n",
    "}\n",
    "if backward_prefetch is not None:\n",
    "    fsdp_kwargs[\"backward_prefetch\"] = backward_prefetch\n",
    "if args.forward_prefetch:\n",
    "    fsdp_kwargs[\"forward_prefetch\"] = True\n",
    "\n",
    "model = FSDP(model, **fsdp_kwargs)\n",
    "logger.info(f\"Created FSDP model with configuration: {fsdp_kwargs}\")\n",
    "```\n",
    "\n",
    "**Changes Made**:\n",
    "- Load the model on CPU first to avoid GPU-specific initialization issues.\n",
    "- Synchronize all parameters (not just `requires_grad` ones) using `dist.broadcast` to ensure consistency.\n",
    "- Move the model to GPU only after synchronization.\n",
    "- Apply FSDP wrapping after synchronization and device placement.\n",
    "\n",
    "#### 2. **Disable `use_orig_params` in FSDP**\n",
    "The `use_orig_params=True` setting in FSDP can sometimes cause issues with parameter synchronization, especially with PEFT models like LoRA. Setting `use_orig_params=False` forces FSDP to create new parameter tensors, which can help ensure consistency.\n",
    "\n",
    "**Fix**: Update the FSDP configuration to set `use_orig_params=False`.\n",
    "\n",
    "**Modified Code** (in the FSDP setup section):\n",
    "\n",
    "```python\n",
    "fsdp_kwargs = {\n",
    "    \"auto_wrap_policy\": auto_wrap_policy,\n",
    "    \"mixed_precision\": mixed_precision_policy,\n",
    "    \"sharding_strategy\": sharding_strategy,\n",
    "    \"limit_all_gathers\": True,\n",
    "    \"cpu_offload\": cpu_offload,\n",
    "    \"device_id\": torch.cuda.current_device(),\n",
    "    \"use_orig_params\": False  # Changed to False\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3. **Verify LoRA Configuration**\n",
    "Ensure that the LoRA configuration is applied identically across all ranks. The current code applies LoRA before synchronization, which is fine, but we need to verify that the LoRA parameters are correctly initialized.\n",
    "\n",
    "**Fix**: Add logging to check the number of parameters after applying LoRA to ensure consistency.\n",
    "\n",
    "**Modified Code** (after applying LoRA):\n",
    "\n",
    "```python\n",
    "logger.info(\"Applying LoRA adapters to model\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "if rank == 0:\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    logger.info(f\"Total parameters: {total_params}, Trainable parameters: {trainable_params}\")\n",
    "```\n",
    "\n",
    "This will help you confirm that the model has the same number of parameters on all ranks before synchronization.\n",
    "\n",
    "#### 4. **Check SMDDP Backend Compatibility**\n",
    "The `smddp` backend (SageMaker Data Parallel) might have specific requirements for model synchronization. The current code uses `dist.broadcast` for synchronization, which is correct, but we should ensure that the SMDDP backend is properly initialized.\n",
    "\n",
    "**Fix**: Verify that the `smddp` backend is correctly set up and that all processes are properly initialized.\n",
    "\n",
    "**Modified Code** (at the start of the script):\n",
    "\n",
    "```python\n",
    "import smdistributed.dataparallel.torch.torch_smddp\n",
    "backend = \"smddp\"\n",
    "dist.init_process_group(backend=backend)\n",
    "logger.info(f\"Initialized distributed training with backend: {backend}, rank: {dist.get_rank()}, world_size: {dist.get_world_size()}\")\n",
    "\n",
    "# Verify SMDDP initialization\n",
    "if not dist.is_initialized():\n",
    "    raise RuntimeError(\"Distributed process group not initialized correctly\")\n",
    "```\n",
    "\n",
    "#### 5. **Ensure Dataset Consistency**\n",
    "The dataset sharding in the code looks correct, but ensure that the dataset is loaded and processed identically across all ranks to avoid indirect effects on training.\n",
    "\n",
    "**Fix**: Add logging to verify dataset sizes after sharding.\n",
    "\n",
    "**Modified Code** (in the dataset loading section):\n",
    "\n",
    "```python\n",
    "logger.info(f\"Rank {dist.get_rank()} has {len(train_dataset)} training samples and {len(eval_dataset)} eval samples\")\n",
    "if dist.is_initialized():\n",
    "    dist.barrier()  # Ensure all ranks have loaded the dataset\n",
    "    train_sizes = [0] * dist.get_world_size()\n",
    "    eval_sizes = [0] * dist.get_world_size()\n",
    "    train_sizes[dist.get_rank()] = len(train_dataset)\n",
    "    eval_sizes[dist.get_rank()] = len(eval_dataset)\n",
    "    dist.all_gather_object(train_sizes, train_sizes[dist.get_rank()])\n",
    "    dist.all_gather_object(eval_sizes, eval_sizes[dist.get_rank()])\n",
    "    if rank == 0:\n",
    "        logger.info(f\"Training dataset sizes across ranks: {train_sizes}\")\n",
    "        logger.info(f\"Evaluation dataset sizes across ranks: {eval_sizes}\")\n",
    "        if len(set(train_sizes)) > 1 or len(set(eval_sizes)) > 1:\n",
    "            logger.warning(\"Inconsistent dataset sizes across ranks detected!\")\n",
    "```\n",
    "\n",
    "This ensures that all ranks have consistent dataset sizes, which can indirectly affect training stability.\n",
    "\n",
    "#### 6. **Clear CUDA Cache Before Training**\n",
    "The error might be caused by residual GPU memory states from previous runs. Clearing the CUDA cache before training can help.\n",
    "\n",
    "**Fix**: Add a call to `clear_device_cache` before model initialization.\n",
    "\n",
    "**Modified Code** (at the start of the `train` function):\n",
    "\n",
    "```python\n",
    "def train(args, device):\n",
    "    clear_device_cache()  # Clear CUDA cache at the start\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    rank = int(os.environ.get(\"RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    logger.info(f\"Distributed training initialized. Rank: {rank}, World size: {world_size}, Backend: {backend}\")\n",
    "```\n",
    "\n",
    "#### 7. **Disable Parameter Flattening (Optional)**\n",
    "The `flatten_parameters` argument is set to `True`, which can sometimes cause issues with FSDP and LoRA. Try disabling it to see if it resolves the issue.\n",
    "\n",
    "**Fix**: Set `flatten_parameters=False` in the argument parser.\n",
    "\n",
    "**Modified Code** (in the argument parser):\n",
    "\n",
    "```python\n",
    "parser.add_argument(\"--flatten_parameters\", action=\"store_true\", default=False, \n",
    "                    help=\"Enable parameter flattening for FSDP (improves performance)\")\n",
    "```\n",
    "\n",
    "Then, comment out or remove the `flatten_parameters` logic in the auto-wrap policy:\n",
    "\n",
    "```python\n",
    "auto_wrap_policy = functools.partial(\n",
    "    transformer_auto_wrap_policy,\n",
    "    transformer_layer_cls={LlamaDecoderLayer}\n",
    ")\n",
    "```\n",
    "\n",
    "#### 8. **Update SageMaker Environment Variables**\n",
    "Ensure that the SageMaker environment variables (`SM_HOSTS`, `SM_CURRENT_HOST`, `SM_MODEL_DIR`, etc.) are correctly set in your SageMaker training job configuration. Misconfigured environment variables can lead to issues with distributed training.\n",
    "\n",
    "**Fix**: Verify your SageMaker training script configuration. For example, ensure that the `SM_NUM_GPUS` and `SM_DEFAULT_BUCKET` environment variables are set correctly in your SageMaker estimator:\n",
    "\n",
    "```python\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"finetune_llama.py\",\n",
    "    role=\"SageMakerRole\",\n",
    "    instance_count=2,  # Number of instances\n",
    "    instance_type=\"ml.p4d.24xlarge\",  # Instance with multiple GPUs\n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    "    hyperparameters={\n",
    "        \"batch-size\": 25,\n",
    "        \"epochs\": 7,\n",
    "        \"lr\": 3e-4,\n",
    "        \"seed\": 1,\n",
    "        \"project-name\": \"vallr-phoneme-llama\",\n",
    "        \"sharding_strategy\": \"FULL_SHARD\",\n",
    "        \"cpu_offload\": True,\n",
    "        \"forward_prefetch\": True,\n",
    "        \"backward_prefetch\": \"BACKWARD_PRE\",\n",
    "        \"activation_checkpointing\": True,\n",
    "        \"min_params_to_wrap\": 10000000\n",
    "    },\n",
    "    environment={\n",
    "        \"SM_DEFAULT_BUCKET\": \"your-s3-bucket\",\n",
    "        \"HF_TOKEN\": \"your-hf-token\"\n",
    "    }\n",
    ")\n",
    "estimator.fit({\"training\": \"s3://your-s3-bucket/data/\"})\n",
    "```\n",
    "\n",
    "### Final Updated Code Snippet\n",
    "Here’s the consolidated updated `train` function incorporating the key fixes:\n",
    "\n",
    "```python\n",
    "def train(args, device):\n",
    "    clear_device_cache()  # Clear CUDA cache\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    rank = int(os.environ.get(\"RANK\", 0))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    logger.info(f\"Distributed training initialized. Rank: {rank}, World size: {world_size}, Backend: {backend}\")\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        raise RuntimeError(\"Distributed process group not initialized correctly\")\n",
    "\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed + rank)\n",
    "        torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # Load tokenizer\n",
    "    model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        token=os.environ.get(\"HF_TOKEN\"),\n",
    "        use_fast=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model configuration\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.use_cache = False\n",
    "\n",
    "    # Determine compute dtype\n",
    "    compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    logger.info(f\"Using {compute_dtype} as compute dtype\")\n",
    "\n",
    "    # Load model on CPU\n",
    "    logger.info(f\"Loading base model: {model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=compute_dtype,\n",
    "        token=os.environ.get(\"HF_TOKEN\"),\n",
    "        device_map=None\n",
    "    )\n",
    "\n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        inference_mode=False\n",
    "    )\n",
    "    logger.info(\"Applying LoRA adapters to model\")\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    if rank == 0:\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        logger.info(f\"Total parameters: {total_params}, Trainable parameters: {trainable_params}\")\n",
    "\n",
    "    # Synchronize model parameters\n",
    "    if dist.is_initialized():\n",
    "        logger.info(f\"Synchronizing model parameters across processes (rank {rank})\")\n",
    "        for param in model.parameters():\n",
    "            dist.broadcast(param.data, src=0)\n",
    "        dist.barrier()\n",
    "        logger.info(f\"Model parameters synchronized on rank {rank}\")\n",
    "\n",
    "    # Move model to GPU\n",
    "    device_id = local_rank\n",
    "    model = model.to(f\"cuda:{device_id}\")\n",
    "    logger.info(f\"Moved model to device cuda:{device_id}\")\n",
    "\n",
    "    # FSDP configuration\n",
    "    bf16_ready = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    mixed_precision_policy = MixedPrecision(\n",
    "        param_dtype=torch.bfloat16 if bf16_ready else torch.float16,\n",
    "        reduce_dtype=torch.bfloat16 if bf16_ready else torch.float16,\n",
    "        buffer_dtype=torch.bfloat16 if bf16_ready else torch.float16\n",
    "    )\n",
    "    cpu_offload = CPUOffload(offload_params=args.cpu_offload)\n",
    "    auto_wrap_policy = functools.partial(\n",
    "        transformer_auto_wrap_policy,\n",
    "        transformer_layer_cls={LlamaDecoderLayer}\n",
    "    )\n",
    "    sharding_strategy = ShardingStrategy.FULL_SHARD if args.sharding_strategy == \"FULL_SHARD\" else ShardingStrategy.SHARD_GRAD_OP\n",
    "    backward_prefetch = {\n",
    "        \"BACKWARD_PRE\": BackwardPrefetch.BACKWARD_PRE,\n",
    "        \"BACKWARD_POST\": BackwardPrefetch.BACKWARD_POST,\n",
    "        \"NONE\": None\n",
    "    }.get(args.backward_prefetch, None)\n",
    "\n",
    "    fsdp_kwargs = {\n",
    "        \"auto_wrap_policy\": auto_wrap_policy,\n",
    "        \"mixed_precision\": mixed_precision_policy,\n",
    "        \"sharding_strategy\": sharding_strategy,\n",
    "        \"limit_all_gathers\": True,\n",
    "        \"cpu_offload\": cpu_offload,\n",
    "        \"device_id\": torch.cuda.current_device(),\n",
    "        \"use_orig_params\": False\n",
    "    }\n",
    "    if backward_prefetch is not None:\n",
    "        fsdp_kwargs[\"backward_prefetch\"] = backward_prefetch\n",
    "    if args.forward_prefetch:\n",
    "        fsdp_kwargs[\"forward_prefetch\"] = True\n",
    "\n",
    "    model = FSDP(model, **fsdp_kwargs)\n",
    "    logger.info(f\"Created FSDP model with configuration: {fsdp_kwargs}\")\n",
    "\n",
    "    # Apply activation checkpointing\n",
    "    if args.activation_checkpointing:\n",
    "        non_reentrant_wrapper = functools.partial(\n",
    "            checkpoint_wrapper,\n",
    "            offload_to_cpu=True,\n",
    "            checkpoint_impl=CheckpointImpl.NO_REENTRANT\n",
    "        )\n",
    "        check_fn = lambda submodule: isinstance(submodule, LlamaDecoderLayer)\n",
    "        apply_activation_checkpointing(\n",
    "            model,\n",
    "            checkpoint_wrapper_fn=non_reentrant_wrapper,\n",
    "            check_fn=check_fn\n",
    "        )\n",
    "        logger.info(f\"Applied activation checkpointing to {model.__class__.__name__}\")\n",
    "\n",
    "    # Log memory usage\n",
    "    if rank == 0:\n",
    "        gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        gpu_memory_reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        logger.info(f\"GPU memory allocated: {gpu_memory_allocated:.2f} GB\")\n",
    "        logger.info(f\"GPU memory reserved: {gpu_memory_reserved:.2f} GB\")\n",
    "\n",
    "    # Load and shard dataset\n",
    "    dataset = PhonemeDataset(args.train_data_dir, output_file=\"phoneme_sentence_pairs.json\")\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No valid data found in dataset\")\n",
    "    hf_dataset = HFDataset.from_list(dataset.data)\n",
    "    tokenized_dataset = hf_dataset.map(\n",
    "        tokenize_and_add_labels,\n",
    "        batched=True,\n",
    "        remove_columns=[\"phonemes\", \"text\", \"input_text\"]\n",
    "    )\n",
    "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = train_test_split[\"train\"]\n",
    "    eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "    if dist.is_initialized() and dist.get_world_size() > 1:\n",
    "        train_dataset = train_dataset.shuffle(seed=args.seed)\n",
    "        eval_dataset = eval_dataset.shuffle(seed=args.seed)\n",
    "        train_dataset = train_dataset.shard(\n",
    "            num_shards=dist.get_world_size(),\n",
    "            index=dist.get_rank()\n",
    "        )\n",
    "        eval_dataset = eval_dataset.shard(\n",
    "            num_shards=dist.get_world_size(),\n",
    "            index=dist.get_rank()\n",
    "        )\n",
    "        logger.info(f\"Rank {dist.get_rank()} has {len(train_dataset)} training samples and {len(eval_dataset)} eval samples\")\n",
    "        dist.barrier()\n",
    "        train_sizes = [0] * dist.get_world_size()\n",
    "        eval_sizes = [0] * dist.get_world_size()\n",
    "        train_sizes[dist.get_rank()] = len(train_dataset)\n",
    "        eval_sizes[dist.get_rank()] = len(eval_dataset)\n",
    "        dist.all_gather_object(train_sizes, train_sizes[dist.get_rank()])\n",
    "        dist.all_gather_object(eval_sizes, eval_sizes[dist.get_rank()])\n",
    "        if rank == 0:\n",
    "            logger.info(f\"Training dataset sizes across ranks: {train_sizes}\")\n",
    "            logger.info(f\"Evaluation dataset sizes across ranks: {eval_sizes}\")\n",
    "            if len(set(train_sizes)) > 1 or len(set(eval_sizes)) > 1:\n",
    "                logger.warning(\"Inconsistent dataset sizes across ranks detected!\")\n",
    "\n",
    "    # Rest of the training code remains unchanged...\n",
    "```\n",
    "\n",
    "### Additional Debugging Steps\n",
    "If the above fixes don’t resolve the issue, try these debugging steps:\n",
    "\n",
    "1. **Log Parameter Sizes**: Add logging to print the size of each parameter before and after synchronization to identify which parameter is causing the mismatch.\n",
    "\n",
    "   ```python\n",
    "   if rank == 0:\n",
    "       for name, param in model.named_parameters():\n",
    "           logger.info(f\"Parameter {name}: size {list(param.size())}, dtype {param.dtype}\")\n",
    "   dist.barrier()\n",
    "   ```\n",
    "\n",
    "2. **Run on a Single GPU**: Temporarily disable distributed training by setting `instance_count=1` in your SageMaker estimator to rule out distributed training issues.\n",
    "\n",
    "3. **Check SageMaker Logs**: Review the full SageMaker training logs for any additional errors or warnings related to model loading, LoRA application, or FSDP initialization.\n",
    "\n",
    "4. **Update Dependencies**: Ensure that your PyTorch, Transformers, and PEFT versions are compatible. For example, use:\n",
    "   - PyTorch 2.0.1 or later\n",
    "   - Transformers 4.36.0 or later\n",
    "   - PEFT 0.7.0 or later\n",
    "\n",
    "   Update your SageMaker estimator to use a compatible framework version:\n",
    "\n",
    "   ```python\n",
    "   estimator = PyTorch(\n",
    "       framework_version=\"2.0.1\",\n",
    "       py_version=\"py310\",\n",
    "       ...\n",
    "   )\n",
    "   ```\n",
    "\n",
    "### Testing the Fixes\n",
    "1. Update your `finetune_llama.py` script with the changes above.\n",
    "2. Redeploy the SageMaker training job with the updated script and verify the environment variables.\n",
    "3. Monitor the logs for parameter counts, dataset sizes, and memory usage to ensure consistency across ranks.\n",
    "4. If the error persists, check the logged parameter sizes to pinpoint the mismatched parameter.\n",
    "\n",
    "### Expected Outcome\n",
    "These changes should resolve the parameter mismatch error by ensuring consistent model initialization, proper synchronization, and correct FSDP configuration. The model should train successfully across all ranks, and the logs will confirm consistent parameter counts and dataset sizes.\n",
    "\n",
    "If you encounter further issues or need help with specific log outputs, please share the relevant logs, and I can provide more targeted assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0586f5",
   "metadata": {},
   "source": [
    "https://github.com/aws/deep-learning-containers/blob/c54da3b9246fe487d2f898f7f0042f25c84ddedf/available_images.md\n",
    "https://github.com/aws/deep-learning-containers/tree/c54da3b9246fe487d2f898f7f0042f25c84ddedf\n",
    "https://github.com/aws/deep-learning-containers/blob/c54da3b9246fe487d2f898f7f0042f25c84ddedf/pytorch/training/docker/2.5/py3/cu124/Dockerfile.gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slip-ml-bXUTykFe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
