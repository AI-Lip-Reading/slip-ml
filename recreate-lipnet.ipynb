{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b27398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import CTCLoss\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import shutil\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logging in to W&B\")\n",
    "secret_name = \"wandb\"\n",
    "region_name = \"us-east-1\"\n",
    "\n",
    "# Create a Secrets Manager client\n",
    "session = boto3.session.Session()\n",
    "secretsmanager = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "\n",
    "get_secret_value_response = secretsmanager.get_secret_value(SecretId=secret_name)\n",
    "\n",
    "secret = get_secret_value_response['SecretString']\n",
    "api_key = json.loads(secret)[\"API_KEY\"]\n",
    "wandb.login(key=api_key)\n",
    "\n",
    "# Initialize W&B project\n",
    "wandb.init(project=\"lipnet-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ac391",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "bucket = 'slip-ml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44eada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_files_from_s3(bucket_name):\n",
    "    \"\"\"\n",
    "    List all files in the specified S3 bucket and download them to a local directory.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "        local_dir (str): Local directory to save the downloaded files.\n",
    "    \"\"\"\n",
    "    local_dir = f\"{os.getcwd()}/input_data\"\n",
    "    # Ensure the local directory exists\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    # loop through folders\n",
    "    for folder in ['video/', 'text/']:\n",
    "        try:\n",
    "            # List all objects in the bucket\n",
    "            response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder)\n",
    "            if 'Contents' not in response:\n",
    "                print(f\"No files found in bucket {bucket_name}.\")\n",
    "                return\n",
    "\n",
    "            for obj in response['Contents'][:20]:\n",
    "                file_key = obj['Key']\n",
    "                local_file_path = os.path.join(local_dir, file_key)\n",
    "\n",
    "                # Ensure subdirectories are created\n",
    "                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "                # Download the file\n",
    "                s3_client.download_file(bucket_name, file_key, local_file_path)\n",
    "\n",
    "            print(\"All files downloaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path, img_w, img_h, frames_n):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while frame_count < frames_n:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize frame\n",
    "        frame = cv2.resize(frame, (img_w, img_h))\n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Normalize pixel values\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Pad with black frames if the video has fewer than `frames_n` frames\n",
    "    while len(frames) < frames_n:\n",
    "        frames.append(np.zeros((img_h, img_w, 3)))\n",
    "\n",
    "    # Convert to numpy array\n",
    "    frames = np.array(frames)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa84738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_labels(text_path, char_to_idx):\n",
    "    # read in json file\n",
    "    with open(text_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Extract the text from the JSON data\n",
    "    text = data['text']\n",
    "\n",
    "    return [char_to_idx[char] for char in text if char in char_to_idx], text\n",
    "\n",
    "# Example character-to-index mapping\n",
    "char_to_idx = {char: idx for idx, char in enumerate(\" abcdefghijklmnopqrstuvwxyz'.?!\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1eb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunked_data(video_dir, text_dir, output_dir, char_to_idx, img_w, img_h, frames_n):\n",
    "    \"\"\"\n",
    "    Preprocess all video and text chunks in the specified directories.\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Directory containing video chunks.\n",
    "        text_dir (str): Directory containing text chunks.\n",
    "        output_dir (str): Directory to save preprocessed data.\n",
    "        char_to_idx (dict): Character-to-index mapping.\n",
    "        img_w (int): Width to resize video frames.\n",
    "        img_h (int): Height to resize video frames.\n",
    "        frames_n (int): Number of frames to extract per video chunk.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get all video and label files\n",
    "    video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])\n",
    "    label_files = sorted([f for f in os.listdir(text_dir) if f.endswith(\".json\")])\n",
    "\n",
    "    for video_file in video_files:\n",
    "        # Extract video ID and chunk number from the filename\n",
    "        video_id, chunk_number = video_file.split(\"__video__\")\n",
    "        chunk_number = chunk_number.split(\".\")[0]\n",
    "\n",
    "        # Find the corresponding label file\n",
    "        label_file = f\"{video_id}__text__{chunk_number}.json\"\n",
    "        if label_file not in label_files:\n",
    "            print(f\"Warning: No matching label file for {video_file}\")\n",
    "            continue\n",
    "\n",
    "        # Full paths for video and label files\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        label_path = os.path.join(text_dir, label_file)\n",
    "\n",
    "        # Preprocess video\n",
    "        frames = preprocess_video(video_path, img_w, img_h, frames_n)\n",
    "\n",
    "        # Convert text to labels\n",
    "        labels, text = text_to_labels(label_path, char_to_idx)\n",
    "\n",
    "        # Save preprocessed video as .npy file\n",
    "        chunk_id = f\"{video_id}__chunk__{chunk_number}\"\n",
    "        video_output_path = os.path.join(output_dir, f\"video/{chunk_id}.npy\")\n",
    "        os.makedirs(output_dir + 'video/', exist_ok=True)\n",
    "        np.save(video_output_path, frames)\n",
    "\n",
    "        # Save labels as .json file\n",
    "        label_output_path = os.path.join(output_dir, f\"text/{chunk_id}.json\")\n",
    "        os.makedirs(output_dir + 'text/', exist_ok=True)\n",
    "        with open(label_output_path, \"w\") as f:\n",
    "            json.dump({\"text\": text, \"labels\": labels}, f)\n",
    "\n",
    "    print(\"Preprocessing complete. Data saved to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNetDataset(Dataset):\n",
    "    def __init__(self, video_dir, label_dir):\n",
    "        \"\"\"\n",
    "        Initialize the LipNetDataset.\n",
    "\n",
    "        Args:\n",
    "            video_dir (str): Directory containing preprocessed video `.npy` files.\n",
    "            label_dir (str): Directory containing preprocessed label `.json` files.\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".npy\")])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith(\".json\")])\n",
    "\n",
    "        # Ensure the number of video and label files match\n",
    "        assert len(self.video_files) == len(self.label_files), \"Mismatch between video and label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (frames, labels), where:\n",
    "                - frames (torch.Tensor): Preprocessed video frames of shape (C, T, H, W).\n",
    "                - labels (torch.Tensor): Corresponding label sequence as a tensor of integers.\n",
    "        \"\"\"\n",
    "        # Load video frames\n",
    "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
    "        frames = np.load(video_path, allow_pickle=True)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = json.load(f)\n",
    "        labels = torch.tensor(label_data[\"labels\"], dtype=torch.long)\n",
    "\n",
    "        return frames, labels\n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save the dataset object for later use.\n",
    "\n",
    "        Args:\n",
    "            save_path (str): Path to save the dataset object.\n",
    "        \"\"\"\n",
    "        torch.save(self, save_path)\n",
    "        print(f\"Dataset saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(load_path):\n",
    "        \"\"\"\n",
    "        Load a saved dataset object.\n",
    "\n",
    "        Args:\n",
    "            load_path (str): Path to the saved dataset object.\n",
    "\n",
    "        Returns:\n",
    "            LipNetDataset: Loaded dataset object.\n",
    "        \"\"\"\n",
    "        dataset = torch.load(load_path)\n",
    "        print(f\"Dataset loaded from {load_path}\")\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ee324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNet(nn.Module):\n",
    "    def __init__(self, img_c, img_w, img_h, frames_n, output_size=31):\n",
    "        super(LipNet, self).__init__()\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.frames_n = frames_n\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # First 3D Convolutional Block\n",
    "        self.conv1 = nn.Conv3d(img_c, 32, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        # Second 3D Convolutional Block\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Third 3D Convolutional Block\n",
    "        self.conv3 = nn.Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "        # GRU layers\n",
    "        feature_map_size = 1728 # To dynamically calculated flattened size: 96 * (img_w // 8) * (img_h // 8)\n",
    "        self.gru1 = nn.GRU(feature_map_size, 256, batch_first=True, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(512, 256, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Dense layer for character predictions\n",
    "        self.fc = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First 3D Convolutional Block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second 3D Convolutional Block\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third 3D Convolutional Block\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Reshape for RNN\n",
    "        batch_size, channels, frames, height, width = x.size()\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # (batch, frames, channels, height, width)\n",
    "        x = x.view(batch_size, frames, -1)  # Flatten height and width\n",
    "\n",
    "        # GRU layers\n",
    "        x, _ = self.gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "\n",
    "        # Dense layer for character predictions\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to handle variable-length sequences in a batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of tuples (frames, labels).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (frames, labels, input_lengths, label_lengths)\n",
    "    \"\"\"\n",
    "    frames, labels = zip(*batch)\n",
    "\n",
    "    # Stack frames into a tensor (batch_size, C, T, H, W)\n",
    "    frames = torch.stack(frames)\n",
    "\n",
    "    # Compute input lengths (number of frames per video)\n",
    "    input_lengths = torch.tensor([frames.size(2)] * len(frames), dtype=torch.long)\n",
    "\n",
    "    # Compute label lengths (number of characters per label sequence)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "\n",
    "    # Concatenate labels into a single tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return frames, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b52b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, labels, label_lengths):\n",
    "    \"\"\"\n",
    "    Calculate accuracy by comparing predictions with ground truth labels.\n",
    "\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted sequences (T, batch_size, output_size).\n",
    "        labels (torch.Tensor): Ground truth labels (concatenated for the batch).\n",
    "        label_lengths (torch.Tensor): Lengths of each label sequence in the batch.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # Decode predictions (get the index of the max log-probability for each time step)\n",
    "    predicted_indices = torch.argmax(predictions, dim=-1).permute(1, 0)  # (batch_size, T)\n",
    "\n",
    "    # Remove blank tokens (assume blank token index is 0)\n",
    "    blank_token = 0\n",
    "    decoded_predictions = []\n",
    "    for pred in predicted_indices:\n",
    "        decoded_sequence = []\n",
    "        prev_token = None\n",
    "        for token in pred:\n",
    "            if token != blank_token and token != prev_token:  # Remove blanks and duplicates\n",
    "                decoded_sequence.append(token.item())\n",
    "            prev_token = token\n",
    "        decoded_predictions.append(decoded_sequence)\n",
    "\n",
    "    # Compare with ground truth labels\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_idx = 0\n",
    "    for i, length in enumerate(label_lengths):\n",
    "        ground_truth = labels[start_idx:start_idx + length].tolist()\n",
    "        start_idx += length\n",
    "        if decoded_predictions[i] == ground_truth:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return (correct / total) * 100 if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126562db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation\n",
    "def train_lipnet_with_validation(model, train_loader, val_loader, epochs, device):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    ctc_loss = CTCLoss(blank=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for frames, labels, input_lengths, label_lengths in train_loader:\n",
    "            frames, labels, input_lengths, label_lengths = (\n",
    "                frames.to(device),\n",
    "                labels.to(device),\n",
    "                input_lengths.to(device),\n",
    "                label_lengths.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames).permute(1, 0, 2)  # (T, batch_size, output_size)\n",
    "            loss = ctc_loss(outputs, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for frames, labels, input_lengths, label_lengths in val_loader:\n",
    "                frames, labels, input_lengths, label_lengths = (\n",
    "                    frames.to(device),\n",
    "                    labels.to(device),\n",
    "                    input_lengths.to(device),\n",
    "                    label_lengths.to(device),\n",
    "                )\n",
    "                outputs = model(frames).permute(1, 0, 2)\n",
    "                loss = ctc_loss(outputs, labels, input_lengths, label_lengths)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                val_accuracy += calculate_accuracy(outputs, labels, label_lengths)\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss / len(train_loader),\n",
    "            \"val_loss\": val_loss / len(val_loader),\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "        })\n",
    "\n",
    "        val_accuracy /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {train_loss / len(train_loader):.4f}, Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "def test_lipnet(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    ctc_loss = CTCLoss(blank=0)\n",
    "    with torch.no_grad():\n",
    "        for frames, labels, input_lengths, label_lengths in test_loader:\n",
    "            frames, labels, input_lengths, label_lengths = (\n",
    "                frames.to(device),\n",
    "                labels.to(device),\n",
    "                input_lengths.to(device),\n",
    "                label_lengths.to(device),\n",
    "            )\n",
    "            outputs = model(frames).permute(1, 0, 2)\n",
    "            loss = ctc_loss(outputs, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            test_accuracy += calculate_accuracy(outputs, labels, label_lengths)\n",
    "\n",
    "    test_accuracy /= len(test_loader)\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_loss\": test_loss / len(test_loader),\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "    })\n",
    "\n",
    "    print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b496fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e274c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_all_files_from_s3(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757498f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_frames(video_dir):\n",
    "    \"\"\"\n",
    "    Calculate the average number of frames across all videos in the directory.\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Directory containing video files.\n",
    "\n",
    "    Returns:\n",
    "        int: Average number of frames (rounded to the nearest integer).\n",
    "    \"\"\"\n",
    "    total_frames = 0\n",
    "    video_count = 0\n",
    "\n",
    "    for video_file in os.listdir(video_dir):\n",
    "        if video_file.endswith(\".mp4\"):  # Ensure it's a video file\n",
    "            video_path = os.path.join(video_dir, video_file)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get the total number of frames\n",
    "            total_frames += frame_count\n",
    "            video_count += 1\n",
    "            cap.release()\n",
    "\n",
    "    if video_count == 0:\n",
    "        raise ValueError(\"No video files found in the directory.\")\n",
    "\n",
    "    return round(total_frames / video_count)  # Return the average number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average number of frames\n",
    "input_video_dir = f\"{os.getcwd()}/input_data/video/\"\n",
    "frames_n = calculate_average_frames(input_video_dir)\n",
    "\n",
    "print(f\"Calculated average frames per video: {frames_n}\")\n",
    "\n",
    "# Set other preprocessing parameters\n",
    "img_w = 1920//3\n",
    "img_h = 1080//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6486a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_dir = f\"{os.getcwd()}/input_data/video/\"\n",
    "input_text_dir = f\"{os.getcwd()}/input_data/text/\"\n",
    "output_dir = f\"{os.getcwd()}/preprocessed_data/\"\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "preprocess_chunked_data(input_video_dir, input_text_dir, output_dir, char_to_idx, img_w=img_w, img_h=img_h, frames_n=frames_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0149d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete input data\n",
    "shutil.rmtree(f\"{os.getcwd()}/input_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb76613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing dataset for training...\")\n",
    "# Define dataset split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Load the dataset\n",
    "dataset = LipNetDataset(video_dir=f\"{os.getcwd()}/preprocessed_data/video/\",\n",
    "                        label_dir=f\"{os.getcwd()}/preprocessed_data/text/\")\n",
    "\n",
    "# Calculate the sizes of each split\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = int(val_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size  # Ensure all samples are used\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Example: Print dataset sizes\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d701be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing model...\")\n",
    "# Initialize the model\n",
    "model = LipNet(img_c=3, img_w=img_w, img_h=img_h, frames_n=frames_n, output_size=len(char_to_idx))\n",
    "\n",
    "print(\"Training model...\")\n",
    "# Train the model\n",
    "model = train_lipnet_with_validation(model, train_loader, val_loader, epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing model...\")\n",
    "test_lipnet(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights to W&B\n",
    "print(\"Saving model weights to W&B...\")\n",
    "torch.save(model.state_dict(), \"lipnet_model.pth\")\n",
    "wandb.save(\"lipnet_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b54d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slip-ml-bXUTykFe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
