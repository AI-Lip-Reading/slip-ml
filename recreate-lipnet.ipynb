{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e2b27398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fa86e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path, img_w=100, img_h=50, frames_n=75):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while frame_count < frames_n:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize frame\n",
    "        frame = cv2.resize(frame, (img_w, img_h))\n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        # Normalize pixel values\n",
    "        frame = frame / 255.0\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Pad with black frames if the video has fewer than `frames_n` frames\n",
    "    while len(frames) < frames_n:\n",
    "        frames.append(np.zeros((img_h, img_w, 3)))\n",
    "\n",
    "    # Convert to numpy array\n",
    "    frames = np.array(frames)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eaa84738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_labels(text_path, char_to_idx):\n",
    "    # read in json file\n",
    "    with open(text_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Extract the text from the JSON data\n",
    "    text = data['text']\n",
    "\n",
    "    return [char_to_idx[char] for char in text if char in char_to_idx], text\n",
    "\n",
    "# Example character-to-index mapping\n",
    "char_to_idx = {char: idx for idx, char in enumerate(\" abcdefghijklmnopqrstuvwxyz'.?!\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b1eb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunked_data(video_dir, text_dir, output_dir, char_to_idx, img_w=100, img_h=50, frames_n=75):\n",
    "    \"\"\"\n",
    "    Preprocess all video and text chunks in the specified directories.\n",
    "\n",
    "    Args:\n",
    "        video_dir (str): Directory containing video chunks.\n",
    "        text_dir (str): Directory containing text chunks.\n",
    "        output_dir (str): Directory to save preprocessed data.\n",
    "        char_to_idx (dict): Character-to-index mapping.\n",
    "        img_w (int): Width to resize video frames.\n",
    "        img_h (int): Height to resize video frames.\n",
    "        frames_n (int): Number of frames to extract per video chunk.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get all video and label files\n",
    "    video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".mp4\")])\n",
    "    label_files = sorted([f for f in os.listdir(text_dir) if f.endswith(\".json\")])\n",
    "\n",
    "    for video_file in video_files:\n",
    "        # Extract video ID and chunk number from the filename\n",
    "        video_id, chunk_number = video_file.split(\"__video__\")\n",
    "        chunk_number = chunk_number.split(\".\")[0]\n",
    "\n",
    "        # Find the corresponding label file\n",
    "        label_file = f\"{video_id}__text__{chunk_number}.json\"\n",
    "        if label_file not in label_files:\n",
    "            print(f\"Warning: No matching label file for {video_file}\")\n",
    "            continue\n",
    "\n",
    "        # Full paths for video and label files\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        label_path = os.path.join(text_dir, label_file)\n",
    "\n",
    "        # Preprocess video\n",
    "        frames = preprocess_video(video_path, img_w, img_h, frames_n)\n",
    "\n",
    "        # Convert text to labels\n",
    "        labels, text = text_to_labels(label_path, char_to_idx)\n",
    "\n",
    "        # Save preprocessed video as .npy file\n",
    "        chunk_id = f\"{video_id}__chunk__{chunk_number}\"\n",
    "        video_output_path = os.path.join(output_dir, f\"videos/{chunk_id}.npy\")\n",
    "        np.save(video_output_path, frames)\n",
    "\n",
    "        # Save labels as .json file\n",
    "        label_output_path = os.path.join(output_dir, f\"labels/{chunk_id}.json\")\n",
    "        with open(label_output_path, \"w\") as f:\n",
    "            json.dump({\"text\": text, \"labels\": labels}, f)\n",
    "\n",
    "        print(f\"Processed and saved: {video_output_path} and {label_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "579ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNetDataset(Dataset):\n",
    "    def __init__(self, video_dir, label_dir):\n",
    "        \"\"\"\n",
    "        Initialize the LipNetDataset.\n",
    "\n",
    "        Args:\n",
    "            video_dir (str): Directory containing preprocessed video `.npy` files.\n",
    "            label_dir (str): Directory containing preprocessed label `.json` files.\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.video_files = sorted([f for f in os.listdir(video_dir) if f.endswith(\".npy\")])\n",
    "        self.label_files = sorted([f for f in os.listdir(label_dir) if f.endswith(\".json\")])\n",
    "\n",
    "        # Ensure the number of video and label files match\n",
    "        assert len(self.video_files) == len(self.label_files), \"Mismatch between video and label files.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (frames, labels), where:\n",
    "                - frames (torch.Tensor): Preprocessed video frames of shape (C, T, H, W).\n",
    "                - labels (torch.Tensor): Corresponding label sequence as a tensor of integers.\n",
    "        \"\"\"\n",
    "        # Load video frames\n",
    "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
    "        frames = np.load(video_path, allow_pickle=True)\n",
    "        frames = torch.tensor(frames, dtype=torch.float32).permute(3, 0, 1, 2)  # (C, T, H, W)\n",
    "\n",
    "        # Load labels\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        with open(label_path, \"r\") as f:\n",
    "            label_data = json.load(f)\n",
    "        labels = torch.tensor(label_data[\"labels\"], dtype=torch.long)\n",
    "\n",
    "        return frames, labels\n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save the dataset object for later use.\n",
    "\n",
    "        Args:\n",
    "            save_path (str): Path to save the dataset object.\n",
    "        \"\"\"\n",
    "        torch.save(self, save_path)\n",
    "        print(f\"Dataset saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(load_path):\n",
    "        \"\"\"\n",
    "        Load a saved dataset object.\n",
    "\n",
    "        Args:\n",
    "            load_path (str): Path to the saved dataset object.\n",
    "\n",
    "        Returns:\n",
    "            LipNetDataset: Loaded dataset object.\n",
    "        \"\"\"\n",
    "        dataset = torch.load(load_path)\n",
    "        print(f\"Dataset loaded from {load_path}\")\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d7ee324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNet(nn.Module):\n",
    "    def __init__(self, img_c=3, img_w=100, img_h=50, frames_n=75, absolute_max_string_len=32, output_size=28):\n",
    "        super(LipNet, self).__init__()\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.frames_n = frames_n\n",
    "        self.absolute_max_string_len = absolute_max_string_len\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # First 3D Convolutional Block\n",
    "        self.conv1 = nn.Conv3d(img_c, 32, kernel_size=(3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        # Second 3D Convolutional Block\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 5, 5), stride=(1, 1, 1), padding=(1, 2, 2))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Third 3D Convolutional Block\n",
    "        self.conv3 = nn.Conv3d(64, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "        # GRU layers\n",
    "        feature_map_size = 1728 # To dynamically calculated flattened size: 96 * (img_w // 8) * (img_h // 8)\n",
    "        self.gru1 = nn.GRU(feature_map_size, 256, batch_first=True, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(512, 256, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Dense layer for character predictions\n",
    "        self.fc = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First 3D Convolutional Block\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second 3D Convolutional Block\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third 3D Convolutional Block\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # Reshape for RNN\n",
    "        batch_size, channels, frames, height, width = x.size()\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # (batch, frames, channels, height, width)\n",
    "        x = x.view(batch_size, frames, -1)  # Flatten height and width\n",
    "\n",
    "        # GRU layers\n",
    "        x, _ = self.gru1(x)\n",
    "        x, _ = self.gru2(x)\n",
    "\n",
    "        # Dense layer for character predictions\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x, dim=-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "555b3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to handle variable-length sequences in a batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of tuples (frames, labels).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (frames, labels, input_lengths, label_lengths)\n",
    "    \"\"\"\n",
    "    frames, labels = zip(*batch)\n",
    "\n",
    "    # Stack frames into a tensor (batch_size, C, T, H, W)\n",
    "    frames = torch.stack(frames)\n",
    "\n",
    "    # Compute input lengths (number of frames per video)\n",
    "    input_lengths = torch.tensor([frames.size(2)] * len(frames), dtype=torch.long)\n",
    "\n",
    "    # Compute label lengths (number of characters per label sequence)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "\n",
    "    # Concatenate labels into a single tensor\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return frames, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "145ba468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train_lipnet(model, dataset, epochs=10, batch_size=8, learning_rate=1e-4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Train the LipNet model using CTC loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The LipNet model.\n",
    "        dataset (Dataset): The dataset containing preprocessed video and label data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        device (str): Device to train on ('cuda' or 'cpu').\n",
    "    \"\"\"\n",
    "    # Move model to the specified device\n",
    "    model.to(device)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    ctc_loss = CTCLoss(blank=0)  # Assuming 0 is the blank token index\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_idx, (frames, labels, input_lengths, label_lengths) in enumerate(dataloader):\n",
    "            # Move data to the specified device\n",
    "            frames = frames.to(device)  # (batch_size, C, T, H, W)\n",
    "            labels = labels.to(device)  # (total_label_length)\n",
    "            input_lengths = input_lengths.to(device)  # (batch_size)\n",
    "            label_lengths = label_lengths.to(device)  # (batch_size)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)  # (batch_size, T, output_size)\n",
    "\n",
    "            # Reshape outputs for CTC loss\n",
    "            outputs = outputs.permute(1, 0, 2)  # (T, batch_size, output_size)\n",
    "\n",
    "            # Compute CTC loss\n",
    "            loss = ctc_loss(outputs, labels, input_lengths, label_lengths)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print epoch loss\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4b496fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "23628173",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ae6486a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__0.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__0.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__1.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__1.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__2.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__2.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__3.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__3.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__4.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__4.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__5.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__5.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__6.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__6.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__7.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__7.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__8.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__8.json\n",
      "Processed and saved: /Users/emmettstorts/Documents/slip-ml/preprocessed_data/videos/KjB6r-HDDI0__chunk__9.npy and /Users/emmettstorts/Documents/slip-ml/preprocessed_data/labels/KjB6r-HDDI0__chunk__9.json\n"
     ]
    }
   ],
   "source": [
    "input_video_dir = f\"{os.getcwd()}/input_data/videos/\"\n",
    "input_text_dir = f\"{os.getcwd()}/input_data/labels/\"\n",
    "output_dir = f\"{os.getcwd()}/preprocessed_data/\"\n",
    "\n",
    "preprocess_chunked_data(input_video_dir, input_text_dir, output_dir, char_to_idx, img_w=100, img_h=50, frames_n=frames_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fb76613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 69.4857\n",
      "Epoch [2/10], Loss: 34.4837\n",
      "Epoch [3/10], Loss: 29.8299\n",
      "Epoch [4/10], Loss: 49.0106\n",
      "Epoch [5/10], Loss: 23.8238\n",
      "Epoch [6/10], Loss: 37.7125\n",
      "Epoch [7/10], Loss: 15.2390\n",
      "Epoch [8/10], Loss: 15.4510\n",
      "Epoch [9/10], Loss: 13.9788\n",
      "Epoch [10/10], Loss: 13.6406\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LipNet(img_c=3, img_w=100, img_h=50, frames_n=frames_n, output_size=len(char_to_idx))\n",
    "\n",
    "# Load the dataset\n",
    "dataset = LipNetDataset(video_dir=f\"{os.getcwd()}/preprocessed_data/videos/\",\n",
    "                        label_dir=f\"{os.getcwd()}/preprocessed_data/labels/\")\n",
    "\n",
    "# Train the model\n",
    "train_lipnet(model, dataset, epochs=10, batch_size=8, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d701be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slip-ml-D1sQfDoh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
