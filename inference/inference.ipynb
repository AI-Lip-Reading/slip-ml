{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f35cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, ViTModel\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import yt_dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdc1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa95bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phoneme vocabulary\n",
    "PHONEME_VOCAB = ['<blank>', 'AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', \n",
    "                 'EH', 'ER', 'EY', 'F', 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L', 'M', 'N', \n",
    "                 'NG', 'OW', 'OY', 'P', 'R', 'S', 'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', \n",
    "                 'Y', 'Z', 'ZH']\n",
    "IDX_TO_PHONEME = {i: p for i, p in enumerate(PHONEME_VOCAB)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b1bdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALLRModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_phonemes=len(PHONEME_VOCAB)):\n",
    "        super(VALLRModel, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.adapter = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(hidden_size, 384, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.Conv1d(384, 192, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.Conv1d(192, 48, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "            torch.nn.Conv1d(48, 16, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.adapter_norm = torch.nn.LayerNorm(16)\n",
    "        self.fc = torch.nn.Linear(16, num_phonemes)\n",
    "\n",
    "    def forward(self, frames):\n",
    "        logger.debug(f\"Input frames shape to VALLRModel: {frames.shape}\")\n",
    "        if len(frames.shape) != 5:\n",
    "            raise ValueError(f\"Expected 5D input tensor [batch_size, num_frames, height, width, channels], got {frames.shape}\")\n",
    "        batch_size, num_frames, h, w, c = frames.shape\n",
    "        frames = frames.permute(0, 1, 4, 2, 3)  # [1, 75, 3, 224, 224]\n",
    "        logger.debug(f\"Frames shape after permute: {frames.shape}\")\n",
    "        frames = frames.view(-1, c, h, w)  # [75, 3, 224, 224]\n",
    "        logger.debug(f\"Frames shape for ViT: {frames.shape}\")\n",
    "        outputs = self.vit(pixel_values=frames)\n",
    "        sequence_output = outputs.last_hidden_state[:, 1:, :]  # [75, 196, 768]\n",
    "        logger.debug(f\"ViT output shape: {sequence_output.shape}\")\n",
    "        # Pool patch dimension (mean across patches)\n",
    "        sequence_output = sequence_output.mean(dim=1)  # [75, 768]\n",
    "        logger.debug(f\"Pooled output shape: {sequence_output.shape}\")\n",
    "        # Reshape to [batch_size, hidden_size, num_frames]\n",
    "        sequence_output = sequence_output.view(batch_size, num_frames, -1).transpose(1, 2)  # [1, 768, 75]\n",
    "        logger.debug(f\"Sequence output shape for adapter: {sequence_output.shape}\")\n",
    "        features = self.adapter(sequence_output)\n",
    "        features = self.adapter_norm(features.transpose(1, 2)).transpose(1, 2)\n",
    "        logits = self.fc(features.transpose(1, 2)).transpose(1, 2)  # [batch, num_phonemes, seq_len]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a511764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face_intervals(video_file):\n",
    "    face_intervals = []\n",
    "    try:\n",
    "        mp_face_detection = mp.solutions.face_detection\n",
    "        face_detection = mp_face_detection.FaceDetection()\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_duration = 1 / fps\n",
    "\n",
    "        face_present = False\n",
    "        start_time = None\n",
    "        no_face_frames = 0\n",
    "        frame_index = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = face_detection.process(rgb_frame)\n",
    "            if results.detections:\n",
    "                if not face_present:\n",
    "                    start_time = frame_index * frame_duration\n",
    "                    face_present = True\n",
    "                no_face_frames = 0\n",
    "            else:\n",
    "                if face_present:\n",
    "                    no_face_frames += 1\n",
    "                    if no_face_frames >= fps * 1.5:\n",
    "                        end_time = frame_index * frame_duration\n",
    "                        face_intervals.append({\"start\": start_time, \"end\": end_time})\n",
    "                        face_present = False\n",
    "            frame_index += 1\n",
    "\n",
    "        if face_present:\n",
    "            end_time = frame_index * frame_duration\n",
    "            face_intervals.append({\"start\": start_time, \"end\": end_time})\n",
    "\n",
    "        cap.release()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in detect_face_intervals: {e}\")\n",
    "        return []\n",
    "    return face_intervals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b14b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video_to_frames(video_path, target_frames=75):\n",
    "    try:\n",
    "        mp_face_detection = mp.solutions.face_detection\n",
    "        face_detection = mp_face_detection.FaceDetection()\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frames = []\n",
    "        frame_indices = []\n",
    "\n",
    "        face_intervals = detect_face_intervals(video_path)\n",
    "        if not face_intervals:\n",
    "            logger.warning(\"No face intervals detected, processing all frames\")\n",
    "            face_intervals = [{\"start\": 0, \"end\": cap.get(cv2.CAP_PROP_FRAME_COUNT) / fps}]\n",
    "\n",
    "        for interval in face_intervals:\n",
    "            start_frame = int(interval[\"start\"] * fps)\n",
    "            end_frame = int(interval[\"end\"] * fps)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "            frame_idx = start_frame\n",
    "            while frame_idx < end_frame:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                results = face_detection.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                if results.detections:\n",
    "                    bbox = results.detections[0].location_data.relative_bounding_box\n",
    "                    h, w = frame.shape[:2]\n",
    "                    x, y = int(bbox.xmin * w), int(bbox.ymin * h)\n",
    "                    width, height = int(bbox.width * w), int(bbox.height * h)\n",
    "                    face = frame[max(0, y):y+height, max(0, x):x+width]\n",
    "                    if face.size > 0:\n",
    "                        face = cv2.resize(face, (224, 224))\n",
    "                        frames.append(face)\n",
    "                        frame_indices.append(frame_idx)\n",
    "                frame_idx += 1\n",
    "        cap.release()\n",
    "\n",
    "        if not frames:\n",
    "            logger.error(\"No valid face frames detected\")\n",
    "            return None\n",
    "        frames_array = np.array(frames)\n",
    "        if len(frames_array) > target_frames:\n",
    "            indices = np.linspace(0, len(frames_array) - 1, target_frames, dtype=int)\n",
    "            frames_array = frames_array[indices]\n",
    "        elif len(frames_array) < target_frames:\n",
    "            pad_length = target_frames - len(frames_array)\n",
    "            frames_array = np.pad(frames_array, ((0, pad_length), (0, 0), (0, 0), (0, 0)), mode=\"edge\")\n",
    "        return frames_array[:target_frames]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in preprocess_video_to_frames: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0422efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, input_data, is_npz=False, transform=None):\n",
    "        if is_npz:\n",
    "            self.frames = np.load(input_data)['frames']\n",
    "        else:\n",
    "            self.frames = preprocess_video_to_frames(input_data)\n",
    "        if self.frames is None:\n",
    "            raise ValueError(\"Failed to preprocess video or load .npz\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.frames\n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(Image.fromarray(frame)) for frame in frames])\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb3e969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_local(model_dir, model_type=\"llama\", use_gpu=True):\n",
    "    device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Loading {model_type} model to {device}\")\n",
    "    \n",
    "    if model_type == \"llama\":\n",
    "        logger.debug(f\"Resolved model_dir: {model_dir}\")\n",
    "        \n",
    "        # Verify required files exist\n",
    "        required_files = ['adapter_config.json', 'adapter_model.safetensors', 'tokenizer.json']\n",
    "        for f in required_files:\n",
    "            if not os.path.exists(os.path.join(model_dir, f)):\n",
    "                raise FileNotFoundError(f\"Required file {f} not found in {model_dir}\")\n",
    "        \n",
    "        # Load tokenizer from the local directory\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "        \n",
    "        # Load base Llama model from Hugging Face\n",
    "        base_model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "        logger.debug(f\"Loading base model: {base_model_name}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            load_in_4bit=use_gpu and torch.cuda.is_available(),\n",
    "            device_map={\"\": 0} if use_gpu and torch.cuda.is_available() else \"cpu\",\n",
    "            token=os.environ.get(\"HF_TOKEN\")  # Use Hugging Face token if needed\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA adapters from the local directory\n",
    "        logger.debug(f\"Loading LoRA adapters from: {model_dir}\")\n",
    "        model = PeftModel.from_pretrained(model, model_dir, local_files_only=True)\n",
    "        model.to(device)\n",
    "        return model, tokenizer\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        model = VALLRModel()\n",
    "        checkpoint_path = os.path.join(model_dir)  # Specific .pth file\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        # Strip 'module.' prefix from DataParallel\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint.items()} if any(k.startswith(\"module.\") for k in checkpoint.keys()) else checkpoint\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        return model, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b3d8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_phonemes(vallr_model, frames, device):\n",
    "    vallr_model.eval()\n",
    "    #frames = frames.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = vallr_model(frames)\n",
    "        phoneme_indices = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    phoneme_sequence = \" \".join([IDX_TO_PHONEME[idx] for idx in phoneme_indices[0] if idx != 0])\n",
    "    return phoneme_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70124852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(llama_model, tokenizer, phoneme_sequence, device):\n",
    "    llama_model.eval()\n",
    "    input_text = f\"{phoneme_sequence} ->\"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_attention_mask=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text.split('->', 1)[-1].strip() if '->' in generated_text else generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4351a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_inference(vallr_model, llama_model, tokenizer, input_data, is_npz=False, use_gpu=True):\n",
    "    device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    dataset = InferenceDataset(input_data, is_npz=is_npz) #, transform=transform)\n",
    "    frames = dataset[0].unsqueeze(0).to(device)\n",
    "    phoneme_sequence = predict_phonemes(vallr_model, frames, device)\n",
    "    text = predict_text(llama_model, tokenizer, phoneme_sequence, device)\n",
    "    return phoneme_sequence, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c449c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_youtube_video_yt_dlp(url):\n",
    "    # extract video ID from the URL\n",
    "    video_id = url.split(\"v=\")[-1]\n",
    "    if \"&\" in video_id:\n",
    "        video_id = video_id.split(\"&\")[0]\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"outtmpl\": f\"{video_id}.%(ext)s\",  # Output path and filename\n",
    "        \"format\": \"best\",  # Select the best single file (video + audio)\n",
    "        \"merge_output_format\": None,  # Avoid merging, stick to single stream\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([url])\n",
    "        print(\"Download completed successfully!\")\n",
    "        return f\"{video_id}.mp4\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f9de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=AJsOA4Zl6Io\n",
      "[youtube] AJsOA4Zl6Io: Downloading webpage\n",
      "[youtube] AJsOA4Zl6Io: Downloading tv client config\n",
      "[youtube] AJsOA4Zl6Io: Downloading player b2858d36-main\n",
      "[youtube] AJsOA4Zl6Io: Downloading tv player API JSON\n",
      "[youtube] AJsOA4Zl6Io: Downloading ios player API JSON\n",
      "[youtube] AJsOA4Zl6Io: Downloading m3u8 information\n",
      "[info] AJsOA4Zl6Io: Downloading 1 format(s): 18\n",
      "[download] Destination: AJsOA4Zl6Io.mp4\n",
      "[download] 100% of    5.41MiB in 00:00:00 at 8.40MiB/s   \n",
      "Download completed successfully!\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'creation_time': '2025-03-04T20:57:47.000000Z', 'encoder': 'Google'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [640, 360], 'bitrate': 270, 'fps': 29.97002997002997, 'codec_name': 'h264', 'profile': '(Main)', 'metadata': {'Metadata': '', 'creation_time': '2025-03-04T20:57:47.000000Z', 'handler_name': 'ISO Media file produced by Google Inc. Created on: 03/04/2025.', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'creation_time': '2025-03-04T20:57:47.000000Z', 'handler_name': 'ISO Media file produced by Google Inc. Created on: 03/04/2025.', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 112.97, 'bitrate': 401, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(Main)', 'video_size': [640, 360], 'video_bitrate': 270, 'video_fps': 29.97002997002997, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 112.97, 'video_n_frames': 3385}\n",
      "/Users/emmettstorts/.local/share/virtualenvs/slip-ml-bXUTykFe/lib/python3.11/site-packages/imageio_ffmpeg/binaries/ffmpeg-macos-aarch64-v7.1 -i AJsOA4Zl6Io.mp4 -loglevel error -f image2pipe -vf scale=640:360 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'mp42', 'minor_version': '0', 'compatible_brands': 'isommp42', 'creation_time': '2025-03-04T20:57:47.000000Z', 'encoder': 'Google'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [640, 360], 'bitrate': 270, 'fps': 29.97002997002997, 'codec_name': 'h264', 'profile': '(Main)', 'metadata': {'Metadata': '', 'creation_time': '2025-03-04T20:57:47.000000Z', 'handler_name': 'ISO Media file produced by Google Inc. Created on: 03/04/2025.', 'vendor_id': '[0][0][0][0]'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': 'eng', 'default': True, 'fps': 44100, 'bitrate': 127, 'metadata': {'Metadata': '', 'creation_time': '2025-03-04T20:57:47.000000Z', 'handler_name': 'ISO Media file produced by Google Inc. Created on: 03/04/2025.', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 112.97, 'bitrate': 401, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(Main)', 'video_size': [640, 360], 'video_bitrate': 270, 'video_fps': 29.97002997002997, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 127, 'video_duration': 112.97, 'video_n_frames': 3385}\n",
      "/Users/emmettstorts/.local/share/virtualenvs/slip-ml-bXUTykFe/lib/python3.11/site-packages/imageio_ffmpeg/binaries/ffmpeg-macos-aarch64-v7.1 -ss 64.000000 -i AJsOA4Zl6Io.mp4 -ss 1.000000 -loglevel error -f image2pipe -vf scale=640:360 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Building video testing_vid.mp4.\n",
      "MoviePy - Writing audio in testing_vidTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video testing_vid.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready testing_vid.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "full_video = 'https://www.youtube.com/watch?v=AJsOA4Zl6Io'\n",
    "video_file = download_youtube_video_yt_dlp(full_video)\n",
    "video = VideoFileClip(video_file)\n",
    "chunk = video.subclipped(65, 72)\n",
    "chunk_filename = f\"testing_vid.mp4\"\n",
    "chunk.write_videofile(chunk_filename, codec=\"libx264\")\n",
    "# delete the local video file\n",
    "os.remove(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d565a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.getcwd() + \"/testing_vid.mp4\" \n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28ab5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vallr model to cpu\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vallr_model_dir = os.getcwd() + \"/vallr_models/model.pth\"\n",
    "vallr_model, _ = load_model_local(vallr_model_dir, model_type=\"vallr\", use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ce2280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading llama model to cpu\n",
      "Resolved model_dir: /Users/emmettstorts/Documents/slip-ml/inference/vallr_models/checkpoint-29750/\n",
      "Loading base model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Loading LoRA adapters from: /Users/emmettstorts/Documents/slip-ml/inference/vallr_models/checkpoint-29750/\n"
     ]
    }
   ],
   "source": [
    "llama_model_dir = os.getcwd() + \"/vallr_models/checkpoint-29750/\"\n",
    "llama_model, tokenizer = load_model_local(llama_model_dir, model_type=\"llama\", use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6894e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747348395.837760 11921918 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1747348395.840836 11922590 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1747348395.846643 11921918 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M1 Pro\n",
      "W0000 00:00:1747348395.847907 11922606 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Input frames shape to VALLRModel: torch.Size([1, 75, 224, 224, 3])\n",
      "Frames shape after permute: torch.Size([1, 75, 3, 224, 224])\n",
      "Frames shape for ViT: torch.Size([75, 3, 224, 224])\n",
      "ViT output shape: torch.Size([75, 196, 768])\n",
      "Pooled output shape: torch.Size([75, 768])\n",
      "Sequence output shape for adapter: torch.Size([1, 768, 75])\n"
     ]
    }
   ],
   "source": [
    "dataset = InferenceDataset(input_file, is_npz=False)\n",
    "frames = torch.tensor(dataset[0]).unsqueeze(0)\n",
    "#frames = frames.permute(0, 1, 4, 2, 3) \n",
    "phoneme_sequence = predict_phonemes(vallr_model, frames, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "112e1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = predict_text(llama_model, tokenizer, phoneme_sequence, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2406b898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"unmoved weddings without doing what you'd want to, travel to the plazma, make more problems out of doing that. what u'd wanted to do, weddings, unmove, move, can't convalve yourself, made more, more. that's it. wow, wow. no, wait, t. w. h. d. unwedded, married, divorced, nobody, no. nobody. wait. welcome, welcome. yes, yes. you, m.m. love, wedding, unmarried,\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split('->', 1)[-1].strip() if '->' in text else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b0e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slip-ml-bXUTykFe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
